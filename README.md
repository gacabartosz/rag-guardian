# RAG Guardian

**PrzestaÅ„ zgadywaÄ‡ czy twÃ³j RAG dziaÅ‚a. Przetestuj go.**

[![License: MIT](https://img.shields.io/badge/License-MIT-yellow.svg)](https://opensource.org/licenses/MIT)
[![Python 3.10+](https://img.shields.io/badge/python-3.10+-blue.svg)](https://www.python.org/downloads/)
[![Tests](https://img.shields.io/badge/tests-119%20passed-brightgreen.svg)](https://github.com/gacabartosz/rag-guardian)
[![Coverage](https://img.shields.io/badge/coverage-68%25-yellow.svg)](https://github.com/gacabartosz/rag-guardian)

---

## Problem ktÃ³ry rozwiÄ…zuje

**Scenariusz:** WdraÅ¼asz system RAG do produkcji. Wszystko dziaÅ‚a... ale:
- Czy model czasami halucynuje?
- Czy uÅ¼ywa wÅ‚aÅ›ciwych dokumentÃ³w?
- Czy retrieval faktycznie znajduje to co powinien?
- Jak to sprawdziÄ‡ **zanim** klient zobaczy bÅ‚Ä™dnÄ… odpowiedÅº?

**Dotychczasowe rozwiÄ…zanie:** RÄ™czne sprawdzanie, modlitwa, nadzieja ğŸ™

**Lepsze rozwiÄ…zanie:** Automatyczne testy jakoÅ›ci RAG. Tak jak pytest dla kodu, RAG Guardian dla systemÃ³w RAG.

## Co to daje w praktyce

Po 30 minutach setupu masz:
- âœ… Automatyczne testy przed kaÅ¼dym wdroÅ¼eniem
- âœ… Metryki ktÃ³re pokazujÄ… **dokÅ‚adnie** co siÄ™ zepsuÅ‚o
- âœ… HTML raporty ktÃ³re moÅ¼esz pokazaÄ‡ szefowi/klientowi
- âœ… Integracja z CI/CD - testy blokujÄ… merge jak coÅ› poszÅ‚o nie tak

**OszczÄ™dnoÅ›Ä‡ czasu:** Zamiast 2h rÄ™cznego sprawdzania przed kaÅ¼dym release â†’ 5 minut automatycznych testÃ³w.

## Jak to dziaÅ‚a - 3 komendy

```bash
# 1. Instalacja
pip install rag-guardian

# 2. Wygeneruj config
rag-guardian init

# 3. Uruchom testy
rag-guardian test --dataset tests/my_test_cases.jsonl
```

**Output:**
```
ğŸš€ RAG Guardian - Starting Evaluation

âœ… Loaded 20 test cases
ğŸ”„ Running evaluation...

============================================================
RAG GUARDIAN - EVALUATION SUMMARY
============================================================

Overall Status: âŒ FAILED (3/20 tests failed)
Pass Rate: 85.0%

METRICS:
âœ… faithfulness        : 0.92 (threshold: 0.85)
âœ… groundedness        : 0.88 (threshold: 0.80)
âŒ context_relevancy   : 0.68 (threshold: 0.75)  â† FIX THIS
âœ… answer_correctness  : 0.90 (threshold: 0.80)

FAILED TESTS:
1. "What's the shipping time?" - retrieval failed (score: 0.68)
2. "Can I cancel my order?" - wrong answer (score: 0.65)
3. "What's your phone number?" - hallucinated (score: 0.71)
============================================================
```

**Wniosek:** Wiesz dokÅ‚adnie co naprawiÄ‡. Nie zgadywanie "coÅ› nie gra".

## API w 10 liniach kodu

```python
from rag_guardian import Evaluator, TestCase

# Test cases (lub zaÅ‚aduj z JSONL)
tests = [
    TestCase(
        question="Jaka jest polityka zwrotÃ³w?",
        expected_answer="30 dni, bez pytaÅ„"
    ),
    TestCase(
        question="Czy wysyÅ‚acie za granicÄ™?",
        expected_answer="Tak, do 50+ krajÃ³w"
    )
]

# Run
evaluator = Evaluator.from_config(".rag-guardian.yml")
results = evaluator.evaluate_dataset(tests)

# Check
if results.passed:
    print("âœ… All good - ship it!")
else:
    print(f"âŒ {results.failed_tests} tests failed - fix before deploy")
    for fail in results.failures:
        print(f"  - {fail.test_case.question}: {fail.failure_reasons}")
```

## Co testuje (4 metryki)

1. **Faithfulness (0-1)** - Czy model siÄ™ nie dopycha? Czy odpowiedÅº opiera siÄ™ na kontekÅ›cie?
2. **Groundedness (0-1)** - Czy faktycznie uÅ¼ywa retrieved docs, czy improwizuje?
3. **Context Relevancy (0-1)** - Czy retrieval znajduje wÅ‚aÅ›ciwe dokumenty?
4. **Answer Correctness (0-1)** - Czy odpowiedÅº zgadza siÄ™ z expected answer?

**KaÅ¼da metryka** ma threshold (np. 0.80). Jak nie przejdzie â†’ test failuje â†’ CI/CD siÄ™ wysypuje â†’ musisz naprawiÄ‡ **zanim** wejdzie na prod.

Prosto. Skutecznie.

## Integracje - dziaÅ‚a z tym co masz

### LangChain (3 linijki)

```python
from langchain.chains import RetrievalQA
from rag_guardian.integrations import LangChainAdapter

qa_chain = RetrievalQA.from_chain_type(...)  # TwÃ³j istniejÄ…cy chain

adapter = LangChainAdapter(qa_chain)
evaluator = Evaluator(adapter)
results = evaluator.evaluate_dataset("tests.jsonl")
```

### LlamaIndex (3 adaptery)

```python
from rag_guardian.integrations import LlamaIndexVectorStoreAdapter

index = VectorStoreIndex.from_documents(documents)  # TwÃ³j index

adapter = LlamaIndexVectorStoreAdapter(index, similarity_top_k=3)
evaluator = Evaluator(adapter)
results = evaluator.evaluate_dataset("tests.jsonl")
```

**Inne adaptery:**
- `LlamaIndexAdapter` - dla dowolnego QueryEngine
- `LlamaIndexChatEngineAdapter` - dla chat RAG

### Custom RAG (wÅ‚asna implementacja)

```python
from rag_guardian.integrations import CustomRAGAdapter

class MyRAG(CustomRAGAdapter):
    def retrieve(self, query: str) -> List[str]:
        return self.vector_db.search(query, top_k=5)

    def generate(self, query: str, contexts: List[str]) -> str:
        return self.llm.generate(f"Context: {contexts}\n\nQ: {query}")

evaluator = Evaluator(MyRAG())
```

**Lub HTTP API:**

```python
from rag_guardian.integrations import CustomHTTPAdapter

adapter = CustomHTTPAdapter(
    endpoint="http://twoj-rag-api.com/query",
    headers={"Authorization": "Bearer token"},
    timeout=30,
    max_retries=3
)
```

## Real-world example - e-commerce customer support

**Setup:**
- RAG na dokumentacji support (100+ FAQs)
- 50 test cases z expected answers
- Threshold: 0.85 dla wszystkich metryk

**Workflow:**
1. Developer zmienia prompt template
2. Push do GitHub
3. GitHub Actions uruchamia `rag-guardian test`
4. Test failuje - faithfulness spadÅ‚o z 0.91 â†’ 0.78
5. PR dostaje âŒ - nie moÅ¼na zmerge'owaÄ‡
6. Developer poprawia prompt
7. Test pass â†’ merge â†’ deploy

**OszczÄ™dnoÅ›Ä‡:** Zamiast klient zgÅ‚asza "coÅ› nie dziaÅ‚a" na produkcji â†’ Å‚apiesz bÅ‚Ä…d **przed** deployem.

**ROI:** Jeden taki bug zÅ‚apany przed prod = kilka godzin oszczÄ™dnoÅ›ci na debugging i hotfix.

## Reporty ktÃ³re moÅ¼esz pokazaÄ‡

### HTML Report (do pokazania szefowi)

```python
from rag_guardian import HTMLReporter

results = evaluator.evaluate_dataset("tests.jsonl")
HTMLReporter.generate(results, "report.html")
```

**Dostajesz:**
- ğŸ“Š Kolorowe wykresy metryk
- ğŸ“ˆ Pass rate w %
- âŒ Lista failed testÃ³w z dokÅ‚adnym powodem
- âœ… Lista passed testÃ³w
- ğŸ“± DziaÅ‚a na telefonie

Otwierasz w przeglÄ…darce, pokazujesz, wszyscy wiedzÄ… co siÄ™ dzieje.

### JSON (do CI/CD)

```python
from rag_guardian import JSONReporter

JSONReporter.save(results, "results.json")
```

Albo z CLI:
```bash
rag-guardian test --dataset tests.jsonl --output-format json
```

Idealnie do parsowania w skryptach, wysyÅ‚ania do Slack, whatever.

## CI/CD - GitHub Actions example

```yaml
name: RAG Quality Tests

on:
  pull_request:
    branches: [ main ]

jobs:
  test-rag:
    runs-on: ubuntu-latest
    steps:
      - uses: actions/checkout@v3

      - name: Setup Python
        uses: actions/setup-python@v4
        with:
          python-version: '3.11'

      - name: Install RAG Guardian
        run: pip install rag-guardian

      - name: Run tests
        run: |
          rag-guardian test \
            --config .rag-guardian.yml \
            --dataset tests/rag_test_cases.jsonl

      - name: Upload results
        if: always()
        uses: actions/upload-artifact@v3
        with:
          name: rag-test-results
          path: results/
```

**Result:** KaÅ¼dy PR automatycznie testowany. Nie mergujesz dopÃ³ki RAG nie przejdzie testÃ³w.

## Co dostajesz (v1.0.0)

- âœ… **4 metryki** - faithfulness, groundedness, context relevancy, answer correctness
- âœ… **CLI + Python API** - uÅ¼ywaj jak ci wygodnie
- âœ… **LangChain + LlamaIndex** - dziaÅ‚a out-of-the-box
- âœ… **Custom integrations** - HTTP adapter lub dziedzicz CustomRAGAdapter
- âœ… **HTML + JSON reports** - dla ludzi i dla maszyn
- âœ… **119 testÃ³w (68% coverage)** - battle-tested
- âœ… **CI/CD ready** - GitHub Actions examples included
- âœ… **Reproducible builds** - poetry.lock commitowany

## Roadmap (co bÄ™dzie)

### v1.1 (styczeÅ„ 2025)
- ğŸ¯ **Semantic similarity** - embeddingi zamiast keyword matching (lepsza accuracy)
- ğŸ“Š **Baseline comparison** - porÃ³wnuj metryki przed/po zmianach
- ğŸ’¬ **Slack notifications** - alert jak testy failujÄ…

### v1.5 (Q1 2025)
- âš¡ **Performance metrics** - latency, token usage, koszty API
- ğŸš€ **Batch processing** - testuj 500+ cases rÃ³wnolegle
- ğŸ’¾ **SQL storage** - zapisuj resulaty do bazy, analizuj trendy

### v2.0 (Q2 2025)
- ğŸ“¡ **Production monitoring** - sample i testuj real user queries
- ğŸ¨ **Web dashboard** - wizualizuj metryki over time
- ğŸ¤– **LLM-as-judge** - GPT-4 do evaluation (dla complex cases)

## Instalacja

### From PyPI
```bash
pip install rag-guardian
```

### From source (deweloperzy)
```bash
git clone https://github.com/gacabartosz/rag-guardian.git
cd rag-guardian

poetry install
poetry run pytest  # 119 testÃ³w powinno przejÅ›Ä‡
```

**Requirements:** Python 3.10+ (testowane na 3.12)

## FAQ - pytania ktÃ³re dostajÄ™

**Q: Jak to siÄ™ rÃ³Å¼ni od Ragas?**

A: Ragas â†’ research-oriented, sporo theory. RAG Guardian â†’ pytest dla RAG, praktyczne testy w CI/CD. Plus mamy first-class LangChain/LlamaIndex support i Å‚adne reporty HTML.

**Q: MuszÄ™ mieÄ‡ running RAG Å¼eby testowaÄ‡?**

A: Tak, RAG Guardian testuje live systems. Ale moÅ¼esz mockowaÄ‡ responses w testach (jak w unit testach).

**Q: Jakie LLM dziaÅ‚a?**

A: Wszystkie. RAG Guardian testuje **system RAG**, nie konkretny model. OpenAI, Anthropic, local Llama - dowolny. Byleby daÅ‚o siÄ™ opakowaÄ‡ w adapter.

**Q: Jak dokÅ‚adne sÄ… metryki?**

A: W v1.0 â†’ keyword matching. Szybkie, ok accuracy (~80-85%). W v1.1 â†’ semantic similarity z embeddings. Wolniejsze, lepsza accuracy (~90-95%).

**Q: Czy mogÄ™ dodaÄ‡ wÅ‚asnÄ… metrykÄ™?**

A: Jasne. Dziedzicz `BaseMetric`, implementuj `evaluate()`. Zobacz [examples/](examples/) jak to zrobiÄ‡.

**Q: Production-ready?**

A: Tak. v1.0 stable, 119 testÃ³w, uÅ¼ywane w prawdziwych projektach. ÅšmiaÅ‚o wrzucaj do CI/CD. Production monitoring (v2.0) to juÅ¼ extra features typu real-time dashboardy.

**Q: Ile to kosztuje?**

A: â‚¬0. Open-source, MIT license. Rob co chcesz.

## Contributing

ZnalazÅ‚eÅ› bug? Masz pomysÅ‚ na feature? PRy mile widziane!

```bash
git clone https://github.com/gacabartosz/rag-guardian.git
cd rag-guardian
poetry install

# Run tests
poetry run pytest

# Format
poetry run black .
poetry run isort .

# Lint
poetry run ruff check .
poetry run mypy rag_guardian
```

## Tech stack

**Built with:**
- [LangChain](https://github.com/langchain-ai/langchain) - RAG framework
- [LlamaIndex](https://github.com/jerryjliu/llama_index) - RAG framework
- [httpx](https://github.com/encode/httpx) - HTTP client with retries

**Inspired by:**
- [Ragas](https://github.com/explodinggradients/ragas) - metrics ideas
- [pytest](https://github.com/pytest-dev/pytest) - testing philosophy

---

## Author

**Made by [Bartosz Gaca](https://bartoszgaca.pl)**

AI & Automation Strategist | BudujÄ™ systemy ktÃ³re oszczÄ™dzajÄ… czas

*ZbudowaÅ‚em RAG Guardian bo miaÅ‚em doÅ›Ä‡ rÄ™cznego sprawdzania czy RAG nie halucynuje przed kaÅ¼dym deployem. Teraz 5 minut `pytest`, raport HTML, git push. Done.*

*JeÅ›li budujesz systemy RAG i chcesz przestaÄ‡ zgadywaÄ‡ czy dziaÅ‚ajÄ… - sprÃ³buj. ZaoszczÄ™dzi ci to godzin.*

---

**License:** MIT - do whatever you want

**Links:**
- ğŸ“¦ [PyPI](https://pypi.org/project/rag-guardian/)
- ğŸ’» [GitHub](https://github.com/gacabartosz/rag-guardian)
- ğŸŒ [Website](https://bartoszgaca.pl)
- ğŸ“§ Contact: [GitHub Issues](https://github.com/gacabartosz/rag-guardian/issues)
